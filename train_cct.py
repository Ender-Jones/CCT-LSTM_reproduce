import argparse
import os
import random
from pathlib import Path
from typing import Dict, Any, List

import numpy as np
import torch
import torch.nn as nn
from sklearn.model_selection import KFold
from torch.utils.data import DataLoader
from tqdm import tqdm

from dataset import SingleImageDataset, get_default_transforms
from model import CCTForPreTraining
from file_path_gen import *


def set_seed(seed: int):
    """
    è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å®éªŒçš„å¯å¤ç°æ€§ã€‚

    Args:
        seed (int): éšæœºç§å­ã€‚
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def train_one_epoch(model: nn.Module, loader: DataLoader, criterion: nn.Module,
                    optimizer: torch.optim.Optimizer, device: torch.device) -> Dict[str, float]:
    """
    å¯¹æ¨¡å‹è¿›è¡Œä¸€ä¸ª epoch çš„è®­ç»ƒã€‚

    Args:
        model (nn.Module): å¾…è®­ç»ƒçš„æ¨¡å‹ã€‚
        loader (DataLoader): è®­ç»ƒæ•°æ®çš„ DataLoaderã€‚
        criterion (nn.Module): æŸå¤±å‡½æ•°ã€‚
        optimizer (torch.optim.Optimizer): ä¼˜åŒ–å™¨ã€‚
        device (torch.device): è®¡ç®—è®¾å¤‡ (cuda/cpu)ã€‚

    Returns:
        Dict[str, float]: åŒ…å«å¹³å‡è®­ç»ƒæŸå¤±ã€å‡†ç¡®ç‡å’Œ F1 åˆ†æ•°çš„å­—å…¸ã€‚
    """
    model.train()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    
    # TODO: åˆå§‹åŒ–ç”¨äºç´¯è®¡æŸå¤±å’Œè¯„ä¼°æŒ‡æ ‡çš„å˜é‡
    # total_loss = 0.0
    # all_preds = []
    # all_labels = []

    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
    progress_bar = tqdm(loader, desc="Training", leave=False)
    for images, labels in progress_bar:
        # TODO: å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        # images = images.to(device)
        # labels = labels.to(device)

        # --- å­¦ä¹ å››æ­¥æ›² ---
        # 1. æ¢¯åº¦æ¸…é›¶
        # optimizer.zero_grad()
        # 2. å‰å‘ä¼ æ’­
        # predictions = model(images)
        # 3. è®¡ç®—æŸå¤±
        # loss = criterion(predictions, labels)
        # 4. åå‘ä¼ æ’­ä¸æƒé‡æ›´æ–°
        # loss.backward()
        # optimizer.step()

        # TODO: ç´¯è®¡æŸå¤±å’Œé¢„æµ‹ç»“æœ/çœŸå®æ ‡ç­¾ï¼Œç”¨äºåç»­æŒ‡æ ‡è®¡ç®—
        # total_loss += loss.item()
        # preds = torch.argmax(predictions, dim=1)
        # all_preds.extend(preds.cpu().numpy())
        # all_labels.extend(labels.cpu().numpy())
        pass

    # TODO: è®¡ç®—æ•´ä¸ª epoch çš„å¹³å‡æŸå¤±å’Œå„é¡¹æŒ‡æ ‡ (accuracy, f1-score)
    # avg_loss = total_loss / len(loader)
    # accuracy = ...
    # f1 = ...

    # return {'train_loss': avg_loss, 'train_acc': accuracy, 'train_f1': f1}
    pass


def validate(model: nn.Module, loader: DataLoader, criterion: nn.Module,
             device: torch.device) -> Dict[str, float]:
    """
    åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

    Args:
        model (nn.Module): å¾…è¯„ä¼°çš„æ¨¡å‹ã€‚
        loader (DataLoader): éªŒè¯æ•°æ®çš„ DataLoaderã€‚
        criterion (nn.Module): æŸå¤±å‡½æ•°ã€‚
        device (torch.device): è®¡ç®—è®¾å¤‡ (cuda/cpu)ã€‚

    Returns:
        Dict[str, float]: åŒ…å«å¹³å‡éªŒè¯æŸå¤±ã€å‡†ç¡®ç‡å’Œ F1 åˆ†æ•°çš„å­—å…¸ã€‚
    """
    model.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

    # TODO: åˆå§‹åŒ–ç”¨äºç´¯è®¡æŸå¤±å’Œè¯„ä¼°æŒ‡æ ‡çš„å˜é‡
    # total_loss = 0.0
    # all_preds = []
    # all_labels = []

    # åœ¨ no_grad ç¯å¢ƒä¸‹è¿›è¡Œï¼Œä»¥èŠ‚çœè®¡ç®—èµ„æº
    with torch.no_grad():
        progress_bar = tqdm(loader, desc="Validating", leave=False)
        for images, labels in progress_bar:
            # TODO: å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            # images = images.to(device)
            # labels = labels.to(device)
            
            # TODO: å‰å‘ä¼ æ’­
            # predictions = model(images)

            # TODO: è®¡ç®—æŸå¤±
            # loss = criterion(predictions, labels)

            # TODO: ç´¯è®¡æŸå¤±å’Œé¢„æµ‹ç»“æœ/çœŸå®æ ‡ç­¾
            # total_loss += loss.item()
            # preds = torch.argmax(predictions, dim=1)
            # all_preds.extend(preds.cpu().numpy())
            # all_labels.extend(labels.cpu().numpy())
            pass

    # TODO: è®¡ç®—æ•´ä¸ªéªŒè¯é›†çš„å¹³å‡æŸå¤±å’Œå„é¡¹æŒ‡æ ‡
    # avg_loss = total_loss / len(loader)
    # accuracy = ...
    # f1 = ...
    
    # return {'val_loss': avg_loss, 'val_acc': accuracy, 'val_f1': f1}
    pass


def main(args: argparse.Namespace):
    """
    ä¸»æ‰§è¡Œå‡½æ•°ï¼Œè´Ÿè´£ç»„ç»‡æ•´ä¸ªäº¤å‰éªŒè¯ã€è®­ç»ƒå’ŒéªŒè¯æµç¨‹ã€‚

    Args:
        args (argparse.Namespace): ä»å‘½ä»¤è¡Œè§£æçš„å‚æ•°ã€‚
    """
    # 1. è®¾ç½®ä¸åˆå§‹åŒ–
    set_seed(args.seed)
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"Weights will be saved to: {output_dir}")
    
    # TODO: å¦‚æœ data_root æ²¡æœ‰è¢«æŒ‡å®šï¼Œå°è¯•ä» UBFC_data_path.txt è¯»å–
    # data_root = Path(args.data_root)

    # 2. æ•°æ®å‡†å¤‡ä¸äº¤å‰éªŒè¯
    # å‡†å¤‡å®Œæ•´çš„è¢«è¯•è€…åˆ—è¡¨
    path_gen = FilePathGen()
    all_subjects = path_gen.get_subject_list()
    kf = KFold(n_splits=7, shuffle=True, random_state=args.seed)

    # TODO: åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨æ¯ä¸ª fold çš„æœ€ä½³ç»“æœï¼Œä»¥ä¾¿æœ€åè®¡ç®—å¹³å‡å€¼
    # all_folds_best_metrics = []

    # 3. äº¤å‰éªŒè¯ä¸»å¾ªç¯
    for fold_idx, (train_indices, val_indices) in enumerate(kf.split(all_subjects)):
        print("\n" + "="*50)
        print(f"Cross-Validation Fold {fold_idx + 1}/7")
        print("="*50)

        # a. åˆ’åˆ†å½“å‰ fold çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_subjects = [all_subjects[i] for i in train_indices]
        val_subjects = [all_subjects[i] for i in val_indices]
        print(f"Training on {len(train_subjects)} subjects.")
        print(f"Validating on {len(val_subjects)} subjects.")
        
        # b. åˆ›å»ºæ•°æ®é›†å’Œ DataLoader
        # TODO: ä½¿ç”¨ train_subjects å’Œ val_subjects åˆ†åˆ«å®ä¾‹åŒ– SingleImageDataset
        # train_dataset = ...
        # val_dataset = ...
        
        # TODO: åˆ›å»º DataLoader
        # train_loader = ...
        # val_loader = ...

        # c. ä¸ºå½“å‰ fold åˆ›å»ºå…¨æ–°çš„æ¨¡å‹ã€ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        # TODO: å®ä¾‹åŒ–æ¨¡å‹å¹¶ç§»åŠ¨åˆ° device
        # model = CCTForPreTraining(num_classes=3).to(device)

        # TODO: å®ä¾‹åŒ–ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        # optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)
        # criterion = nn.CrossEntropyLoss()

        # d. è®­ç»ƒä¸éªŒè¯å¾ªç¯
        # TODO: åˆå§‹åŒ–ç”¨äºè¿½è¸ªå½“å‰ fold æœ€ä½³æ€§èƒ½çš„å˜é‡
        # best_val_f1 = 0.0
        
        for epoch in range(args.epochs):
            print(f"\nEpoch {epoch + 1}/{args.epochs}")

            # TODO: è°ƒç”¨è®­ç»ƒå‡½æ•°
            # train_metrics = train_one_epoch(...)
            
            # TODO: è°ƒç”¨éªŒè¯å‡½æ•°
            # val_metrics = validate(...)

            # TODO: æ‰“å°å½“å‰ epoch çš„è®­ç»ƒå’ŒéªŒè¯ç»“æœ
            # print(f"Train Loss: {train_metrics['train_loss']:.4f}, Train Acc: {train_metrics['train_acc']:.4f}")
            # print(f"Val Loss: {val_metrics['val_loss']:.4f}, Val Acc: {val_metrics['val_acc']:.4f}, Val F1: {val_metrics['val_f1']:.4f}")

            # e. æ£€æŸ¥å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
            # current_f1 = val_metrics['val_f1']
            # if current_f1 > best_val_f1:
            #    best_val_f1 = current_f1
            #    save_path = output_dir / f"cct_{args.modality}_fold_{fold_idx+1}_best.pth"
            #    torch.save(model.state_dict(), save_path)
            #    print(f"ğŸš€ New best model saved to {save_path} with F1 score: {best_val_f1:.4f}")
            pass
        
        # TODO: è®°å½•å½“å‰ fold çš„æœ€ä½³ F1 åˆ†æ•°
        # all_folds_best_metrics.append({'fold': fold_idx + 1, 'best_f1': best_val_f1})
        # print(f"\nBest F1 score for fold {fold_idx + 1}: {best_val_f1:.4f}")
        pass

    # 4. æ€»ç»“å¹¶æ‰“å°æ‰€æœ‰ fold çš„å¹³å‡æ€§èƒ½
    # TODO: è®¡ç®—å¹¶æ‰“å°äº¤å‰éªŒè¯çš„å¹³å‡ F1 åˆ†æ•°
    # avg_f1 = np.mean([m['best_f1'] for m in all_folds_best_metrics])
    # print("\n" + "="*50)
    # print("Cross-Validation Finished!")
    # print(f"Average F1 score across 7 folds: {avg_f1:.4f}")
    # print("="*50)
    pass


if __name__ == '__main__':
    # é…ç½®å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="Stage 1: CCT Pre-training Script")

    # æ ¸å¿ƒå‚æ•°
    parser.add_argument('--modality', type=str, required=True, choices=['landmark', 'rppg'],
                        help="The modality to train on ('landmark' or 'rppg').")
    parser.add_argument('--data-root', type=str, default=None,
                        help="Path to the root of the dataset (e.g., '.../UBFC-Phys/Data'). "
                             "If not provided, will try to read from 'UBFC_data_path.txt'.")
    parser.add_argument('--output-dir', type=str, default='weights',
                        help="Directory to save the best model weights.")

    # è®­ç»ƒè¶…å‚æ•°
    parser.add_argument('--epochs', type=int, default=100,
                        help="Number of training epochs.")
    parser.add_argument('--batch-size', type=int, default=16,
                        help="Batch size for training and validation.")
    parser.add_argument('--lr', type=float, default=1e-4,
                        help="Learning rate for the optimizer.")

    # ç¯å¢ƒä¸å¤ç°æ€§å‚æ•°
    parser.add_argument('--seed', type=int, default=42,
                        help="Random seed for reproducibility.")
    parser.add_argument('--device', type=str, default='cuda',
                        help="Device to use for training ('cuda' or 'cpu').")

    args = parser.parse_args()

    main(args)

